{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/datasets/home/23/223/rmanandi/text-to-image-using-GAN/\")\n",
    "import nbimporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import yaml\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from txt2image_dataset import Text2ImageDataset    #txt2image_dataset.ipynb file\n",
    "from models.gan_factory import gan_factory         #models/gan_factory.ipynb file\n",
    "# from utils import Utils, Logger    #utils.ipynb file\n",
    "import utils\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    #diter argument only for wgan\n",
    "    def __init__(self, type, dataset, split, lr, diter, vis_screen, save_path, l1_coef, l2_coef, pre_trained_gen, pre_trained_disc, batch_size, num_workers, epochs):\n",
    "        with open('config.yaml', 'r') as f:\n",
    "            config = yaml.load(f)\n",
    "\n",
    "        self.generator = torch.nn.DataParallel(gan_factory.generator_factory(type).cuda())\n",
    "        self.discriminator = torch.nn.DataParallel(gan_factory.discriminator_factory(type).cuda())\n",
    "\n",
    "        if pre_trained_disc:\n",
    "            self.discriminator.load_state_dict(torch.load(pre_trained_disc))\n",
    "        else:\n",
    "            self.discriminator.apply(utils.Utils.weights_init)\n",
    "\n",
    "        if pre_trained_gen:\n",
    "            self.generator.load_state_dict(torch.load(pre_trained_gen))\n",
    "        else:\n",
    "            self.generator.apply(utils.Utils.weights_init)\n",
    "\n",
    "        if dataset == 'birds':\n",
    "            self.dataset = Text2ImageDataset(config['birds_dataset_path'], split=split)\n",
    "        elif dataset == 'flowers':\n",
    "            self.dataset = Text2ImageDataset(config['flowers_dataset_path'], split=split)\n",
    "        else:\n",
    "            print('Dataset not supported, please select either birds or flowers.')\n",
    "            exit()\n",
    "\n",
    "        self.noise_dim = 100\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.lr = lr\n",
    "        self.beta1 = 0.5\n",
    "        self.num_epochs = epochs\n",
    "        self.DITER = diter   #diter only for wgan\n",
    "\n",
    "        self.l1_coef = l1_coef\n",
    "        self.l2_coef = l2_coef\n",
    "\n",
    "        self.data_loader = DataLoader(self.dataset, batch_size=self.batch_size, shuffle=True,\n",
    "                                num_workers=self.num_workers)\n",
    "\n",
    "        self.optimD = torch.optim.Adam(self.discriminator.parameters(), lr=self.lr, betas=(self.beta1, 0.999))\n",
    "        self.optimG = torch.optim.Adam(self.generator.parameters(), lr=self.lr, betas=(self.beta1, 0.999))\n",
    "\n",
    "        self.logger = utils.Logger(vis_screen)\n",
    "        self.checkpoints_path = 'checkpoints'\n",
    "        self.save_path = save_path\n",
    "        self.type = type\n",
    "\n",
    "    def train(self, cls=False):\n",
    "\n",
    "        #if self.type == 'wgan':\n",
    "        #    self._train_wgan(cls)\n",
    "        if self.type == 'gan':\n",
    "            self._train_gan(cls)\n",
    "        #elif self.type == 'vanilla_wgan':\n",
    "        #    self._train_vanilla_wgan()\n",
    "        elif self.type == 'vanilla_gan':\n",
    "            self._train_vanilla_gan()      #Vanilla We are not using\n",
    "\n",
    "    '''def _train_wgan(self, cls):\n",
    "        one = torch.FloatTensor([1])\n",
    "        mone = one * -1\n",
    "\n",
    "        one = Variable(one).cuda()\n",
    "        mone = Variable(mone).cuda()\n",
    "\n",
    "        gen_iteration = 0\n",
    "        for epoch in range(self.num_epochs):\n",
    "            iterator = 0\n",
    "            data_iterator = iter(self.data_loader)\n",
    "\n",
    "            while iterator < len(self.data_loader):\n",
    "\n",
    "                if gen_iteration < 25 or gen_iteration % 500 == 0:\n",
    "                    d_iter_count = 100\n",
    "                else:\n",
    "                    d_iter_count = self.DITER\n",
    "\n",
    "                d_iter = 0\n",
    "\n",
    "                # Train the discriminator\n",
    "                while d_iter < d_iter_count and iterator < len(self.data_loader):\n",
    "                    d_iter += 1\n",
    "\n",
    "                    for p in self.discriminator.parameters():\n",
    "                        p.requires_grad = True\n",
    "\n",
    "                    self.discriminator.zero_grad()\n",
    "\n",
    "                    sample = next(data_iterator)\n",
    "                    iterator += 1\n",
    "\n",
    "                    right_images = sample['right_images']\n",
    "                    right_embed = sample['right_embed']\n",
    "                    wrong_images = sample['wrong_images']\n",
    "\n",
    "                    right_images = Variable(right_images.float()).cuda()\n",
    "                    right_embed = Variable(right_embed.float()).cuda()\n",
    "                    wrong_images = Variable(wrong_images.float()).cuda()\n",
    "\n",
    "                    outputs, _ = self.discriminator(right_images, right_embed)\n",
    "                    real_loss = torch.mean(outputs)\n",
    "                    real_loss.backward(mone)\n",
    "\n",
    "                    if cls:\n",
    "                        outputs, _ = self.discriminator(wrong_images, right_embed)\n",
    "                        wrong_loss = torch.mean(outputs)\n",
    "                        wrong_loss.backward(one)\n",
    "\n",
    "                    noise = Variable(torch.randn(right_images.size(0), self.noise_dim), volatile=True).cuda()\n",
    "                    noise = noise.view(noise.size(0), self.noise_dim, 1, 1)\n",
    "\n",
    "                    fake_images = Variable(self.generator(right_embed, noise).data)\n",
    "                    outputs, _ = self.discriminator(fake_images, right_embed)\n",
    "                    fake_loss = torch.mean(outputs)\n",
    "                    fake_loss.backward(one)\n",
    "\n",
    "                    ## NOTE: Pytorch had a bug with gradient penalty at the time of this project development\n",
    "                    ##       , uncomment the next two lines and remove the params clamping below if you want to try gradient penalty\n",
    "                    # gp = Utils.compute_GP(self.discriminator, right_images.data, right_embed, fake_images.data, LAMBDA=10)\n",
    "                    # gp.backward()\n",
    "\n",
    "                    d_loss = real_loss - fake_loss\n",
    "\n",
    "                    if cls:\n",
    "                        d_loss = d_loss - wrong_loss\n",
    "\n",
    "                    self.optimD.step()\n",
    "\n",
    "                    for p in self.discriminator.parameters():\n",
    "                        p.data.clamp_(-0.01, 0.01)\n",
    "\n",
    "                # Train Generator\n",
    "                for p in self.discriminator.parameters():\n",
    "                    p.requires_grad = False\n",
    "                self.generator.zero_grad()\n",
    "                noise = Variable(torch.randn(right_images.size(0), 100)).cuda()\n",
    "                noise = noise.view(noise.size(0), 100, 1, 1)\n",
    "                fake_images = self.generator(right_embed, noise)\n",
    "                outputs, _ = self.discriminator(fake_images, right_embed)\n",
    "\n",
    "                g_loss = torch.mean(outputs)\n",
    "                g_loss.backward(mone)\n",
    "                g_loss = - g_loss\n",
    "                self.optimG.step()\n",
    "\n",
    "                gen_iteration += 1\n",
    "\n",
    "                self.logger.draw(right_images, fake_images)\n",
    "                self.logger.log_iteration_wgan(epoch, gen_iteration, d_loss, g_loss, real_loss, fake_loss)\n",
    "                \n",
    "            self.logger.plot_epoch(gen_iteration)\n",
    "\n",
    "            if (epoch+1) % 50 == 0:\n",
    "                Utils.save_checkpoint(self.discriminator, self.generator, self.checkpoints_path, epoch)'''\n",
    "\n",
    "    def _train_gan(self, cls):\n",
    "        criterion = nn.BCELoss()\n",
    "        l2_loss = nn.MSELoss()\n",
    "        l1_loss = nn.L1Loss()\n",
    "        iteration = 0\n",
    "\n",
    "        for epoch in range(self.num_epochs):\n",
    "            for sample in self.data_loader:\n",
    "                iteration += 1\n",
    "                right_images = sample['right_images']\n",
    "                right_embed = sample['right_embed']\n",
    "                wrong_images = sample['wrong_images']\n",
    "\n",
    "                right_images = Variable(right_images.float()).cuda()\n",
    "                right_embed = Variable(right_embed.float()).cuda()\n",
    "                wrong_images = Variable(wrong_images.float()).cuda()\n",
    "\n",
    "                real_labels = torch.ones(right_images.size(0))\n",
    "                fake_labels = torch.zeros(right_images.size(0))\n",
    "\n",
    "                # ======== One sided label smoothing ==========\n",
    "                # Helps preventing the discriminator from overpowering the\n",
    "                # generator adding penalty when the discriminator is too confident\n",
    "                # =============================================\n",
    "                smoothed_real_labels = torch.FloatTensor(utils.Utils.smooth_label(real_labels.numpy(), -0.1))\n",
    "\n",
    "                real_labels = Variable(real_labels).cuda()\n",
    "                smoothed_real_labels = Variable(smoothed_real_labels).cuda()\n",
    "                fake_labels = Variable(fake_labels).cuda()\n",
    "\n",
    "                # Train the discriminator\n",
    "                self.discriminator.zero_grad()\n",
    "                outputs, activation_real = self.discriminator(right_images, right_embed)\n",
    "                real_loss = criterion(outputs, smoothed_real_labels)\n",
    "                real_score = outputs\n",
    "\n",
    "                if cls:\n",
    "                    outputs, _ = self.discriminator(wrong_images, right_embed)\n",
    "                    wrong_loss = criterion(outputs, fake_labels)\n",
    "                    wrong_score = outputs\n",
    "\n",
    "                noise = Variable(torch.randn(right_images.size(0), 100)).cuda()\n",
    "                noise = noise.view(noise.size(0), 100, 1, 1)\n",
    "                fake_images = self.generator(right_embed, noise)\n",
    "                outputs, _ = self.discriminator(fake_images, right_embed)\n",
    "                fake_loss = criterion(outputs, fake_labels)\n",
    "                fake_score = outputs\n",
    "\n",
    "                d_loss = real_loss + fake_loss\n",
    "\n",
    "                if cls:\n",
    "                    d_loss = d_loss + wrong_loss\n",
    "\n",
    "                d_loss.backward()\n",
    "                self.optimD.step()\n",
    "\n",
    "                # Train the generator\n",
    "                self.generator.zero_grad()\n",
    "                noise = Variable(torch.randn(right_images.size(0), 100)).cuda()\n",
    "                noise = noise.view(noise.size(0), 100, 1, 1)\n",
    "                fake_images = self.generator(right_embed, noise)\n",
    "                outputs, activation_fake = self.discriminator(fake_images, right_embed)\n",
    "                _, activation_real = self.discriminator(right_images, right_embed)\n",
    "\n",
    "                activation_fake = torch.mean(activation_fake, 0)    #try with median and check if it converges\n",
    "                activation_real = torch.mean(activation_real, 0)    #try with median and check if it converges\n",
    "\n",
    "\n",
    "                #======= Generator Loss function============\n",
    "                # This is a customized loss function, the first term is the regular cross entropy loss\n",
    "                # The second term is feature matching loss, this measure the distance between the real and generated\n",
    "                # images statistics by comparing intermediate layers activations\n",
    "                # The third term is L1 distance between the generated and real images, this is helpful for the conditional case\n",
    "                # because it links the embedding feature vector directly to certain pixel values.\n",
    "                #===========================================\n",
    "                g_loss = criterion(outputs, real_labels) \\\n",
    "                         + self.l2_coef * l2_loss(activation_fake, activation_real.detach()) \\\n",
    "                         + self.l1_coef * l1_loss(fake_images, right_images)\n",
    "\n",
    "                g_loss.backward()\n",
    "                self.optimG.step()\n",
    "\n",
    "                if iteration % 5 == 0:\n",
    "                    self.logger.log_iteration_gan(epoch,d_loss, g_loss, real_score, fake_score)\n",
    "                    self.logger.draw(right_images, fake_images)\n",
    "\n",
    "            self.logger.plot_epoch_w_scores(epoch)\n",
    "\n",
    "            if (epoch) % 10 == 0:\n",
    "                utils.Utils.save_checkpoint(self.discriminator, self.generator, self.checkpoints_path, self.save_path, epoch)\n",
    "\n",
    "    '''def _train_vanilla_wgan(self):\n",
    "        one = Variable(torch.FloatTensor([1])).cuda()\n",
    "        mone = one * -1\n",
    "        gen_iteration = 0\n",
    "\n",
    "        for epoch in range(self.num_epochs):\n",
    "         iterator = 0\n",
    "         data_iterator = iter(self.data_loader)\n",
    "\n",
    "         while iterator < len(self.data_loader):\n",
    "\n",
    "             if gen_iteration < 25 or gen_iteration % 500 == 0:\n",
    "                 d_iter_count = 100\n",
    "             else:\n",
    "                 d_iter_count = self.DITER\n",
    "\n",
    "             d_iter = 0\n",
    "\n",
    "             # Train the discriminator\n",
    "             while d_iter < d_iter_count and iterator < len(self.data_loader):\n",
    "                 d_iter += 1\n",
    "\n",
    "                 for p in self.discriminator.parameters():\n",
    "                     p.requires_grad = True\n",
    "\n",
    "                 self.discriminator.zero_grad()\n",
    "\n",
    "                 sample = next(data_iterator)\n",
    "                 iterator += 1\n",
    "\n",
    "                 right_images = sample['right_images']\n",
    "                 right_images = Variable(right_images.float()).cuda()\n",
    "\n",
    "                 outputs, _ = self.discriminator(right_images)\n",
    "                 real_loss = torch.mean(outputs)\n",
    "                 real_loss.backward(mone)\n",
    "\n",
    "                 noise = Variable(torch.randn(right_images.size(0), self.noise_dim), volatile=True).cuda()\n",
    "                 noise = noise.view(noise.size(0), self.noise_dim, 1, 1)\n",
    "\n",
    "                 fake_images = Variable(self.generator(noise).data)\n",
    "                 outputs, _ = self.discriminator(fake_images)\n",
    "                 fake_loss = torch.mean(outputs)\n",
    "                 fake_loss.backward(one)\n",
    "\n",
    "                 ## NOTE: Pytorch had a bug with gradient penalty at the time of this project development\n",
    "                 ##       , uncomment the next two lines and remove the params clamping below if you want to try gradient penalty\n",
    "                 # gp = Utils.compute_GP(self.discriminator, right_images.data, right_embed, fake_images.data, LAMBDA=10)\n",
    "                 # gp.backward()\n",
    "\n",
    "                 d_loss = real_loss - fake_loss\n",
    "                 self.optimD.step()\n",
    "\n",
    "                 for p in self.discriminator.parameters():\n",
    "                     p.data.clamp_(-0.01, 0.01)\n",
    "\n",
    "             # Train Generator\n",
    "             for p in self.discriminator.parameters():\n",
    "                 p.requires_grad = False\n",
    "\n",
    "             self.generator.zero_grad()\n",
    "             noise = Variable(torch.randn(right_images.size(0), 100)).cuda()\n",
    "             noise = noise.view(noise.size(0), 100, 1, 1)\n",
    "             fake_images = self.generator(noise)\n",
    "             outputs, _ = self.discriminator(fake_images)\n",
    "\n",
    "             g_loss = torch.mean(outputs)\n",
    "             g_loss.backward(mone)\n",
    "             g_loss = - g_loss\n",
    "             self.optimG.step()\n",
    "\n",
    "             gen_iteration += 1\n",
    "\n",
    "             self.logger.draw(right_images, fake_images)\n",
    "             self.logger.log_iteration_wgan(epoch, gen_iteration, d_loss, g_loss, real_loss, fake_loss)\n",
    "\n",
    "         self.logger.plot_epoch(gen_iteration)\n",
    "\n",
    "         if (epoch + 1) % 50 == 0:\n",
    "             Utils.save_checkpoint(self.discriminator, self.generator, self.checkpoints_path, epoch)'''\n",
    "\n",
    "    def _train_vanilla_gan(self):\n",
    "        criterion = nn.BCELoss()\n",
    "        l2_loss = nn.MSELoss()\n",
    "        l1_loss = nn.L1Loss()\n",
    "        iteration = 0\n",
    "\n",
    "        for epoch in range(self.num_epochs):\n",
    "            for sample in self.data_loader:\n",
    "                iteration += 1\n",
    "                right_images = sample['right_images']\n",
    "\n",
    "                right_images = Variable(right_images.float()).cuda()\n",
    "\n",
    "                real_labels = torch.ones(right_images.size(0))\n",
    "                fake_labels = torch.zeros(right_images.size(0))\n",
    "\n",
    "                # ======== One sided label smoothing ==========\n",
    "                # Helps preventing the discriminator from overpowering the\n",
    "                # generator adding penalty when the discriminator is too confident\n",
    "                # =============================================\n",
    "                smoothed_real_labels = torch.FloatTensor(utils.Utils.smooth_label(real_labels.numpy(), -0.1))\n",
    "\n",
    "                real_labels = Variable(real_labels).cuda()\n",
    "                smoothed_real_labels = Variable(smoothed_real_labels).cuda()\n",
    "                fake_labels = Variable(fake_labels).cuda()\n",
    "\n",
    "\n",
    "                # Train the discriminator\n",
    "                self.discriminator.zero_grad()\n",
    "                outputs, activation_real = self.discriminator(right_images)\n",
    "                real_loss = criterion(outputs, smoothed_real_labels)\n",
    "                real_score = outputs\n",
    "\n",
    "                noise = Variable(torch.randn(right_images.size(0), 100)).cuda()\n",
    "                noise = noise.view(noise.size(0), 100, 1, 1)\n",
    "                fake_images = self.generator(noise)\n",
    "                outputs, _ = self.discriminator(fake_images)\n",
    "                fake_loss = criterion(outputs, fake_labels)\n",
    "                fake_score = outputs\n",
    "\n",
    "                d_loss = real_loss + fake_loss\n",
    "\n",
    "                d_loss.backward()\n",
    "                self.optimD.step()\n",
    "\n",
    "                # Train the generator\n",
    "                self.generator.zero_grad()\n",
    "                noise = Variable(torch.randn(right_images.size(0), 100)).cuda()\n",
    "                noise = noise.view(noise.size(0), 100, 1, 1)\n",
    "                fake_images = self.generator(noise)\n",
    "                outputs, activation_fake = self.discriminator(fake_images)\n",
    "                _, activation_real = self.discriminator(right_images)\n",
    "\n",
    "                activation_fake = torch.mean(activation_fake, 0)    #try median \n",
    "                activation_real = torch.mean(activation_real, 0)    #try median \n",
    "\n",
    "                # ======= Generator Loss function============\n",
    "                # This is a customized loss function, the first term is the regular cross entropy loss\n",
    "                # The second term is feature matching loss, this measure the distance between the real and generated\n",
    "                # images statistics by comparing intermediate layers activations\n",
    "                # The third term is L1 distance between the generated and real images, this is helpful for the conditional case\n",
    "                # because it links the embedding feature vector directly to certain pixel values.\n",
    "                g_loss = criterion(outputs, real_labels) \\\n",
    "                         + self.l2_coef * l2_loss(activation_fake, activation_real.detach()) \\\n",
    "                         + self.l1_coef * l1_loss(fake_images, right_images)\n",
    "\n",
    "                g_loss.backward()\n",
    "                self.optimG.step()\n",
    "\n",
    "                if iteration % 5 == 0:\n",
    "                    self.logger.log_iteration_gan(epoch, d_loss, g_loss, real_score, fake_score)\n",
    "                    self.logger.draw(right_images, fake_images)\n",
    "\n",
    "            self.logger.plot_epoch_w_scores(iteration)\n",
    "\n",
    "            if (epoch) % 50 == 0:\n",
    "                utils.Utils.save_checkpoint(self.discriminator, self.generator, self.checkpoints_path, epoch)\n",
    "\n",
    "    def predict(self):\n",
    "        for sample in self.data_loader:\n",
    "            right_images = sample['right_images']\n",
    "            right_embed = sample['right_embed']\n",
    "            txt = sample['txt']\n",
    "\n",
    "            if not os.path.exists('results/{0}'.format(self.save_path)):\n",
    "                os.makedirs('results/{0}'.format(self.save_path))\n",
    "\n",
    "            right_images = Variable(right_images.float()).cuda()\n",
    "            right_embed = Variable(right_embed.float()).cuda()\n",
    "\n",
    "            # Train the generator\n",
    "            noise = Variable(torch.randn(right_images.size(0), 100)).cuda()\n",
    "            noise = noise.view(noise.size(0), 100, 1, 1)\n",
    "            fake_images = self.generator(right_embed, noise)\n",
    "\n",
    "            self.logger.draw(right_images, fake_images)\n",
    "\n",
    "            for image, t in zip(fake_images, txt):\n",
    "                im = Image.fromarray(image.data.mul_(127.5).add_(127.5).byte().permute(1, 2, 0).cpu().numpy())\n",
    "                im.save('results/{0}/{1}.jpg'.format(self.save_path, t.replace(\"/\", \"\")[:100]))\n",
    "                print(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
