{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nbimporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Jupyter notebook from visualize.ipynb\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch import  autograd\n",
    "import torch\n",
    "from visualize import VisdomPlotter\n",
    "import os\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Concat_embed(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, projected_embed_dim):\n",
    "        super(Concat_embed, self).__init__()\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(in_features=embed_dim, out_features=projected_embed_dim),\n",
    "            nn.BatchNorm1d(num_features=projected_embed_dim),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
    "            )\n",
    "\n",
    "    def forward(self, inp, embed):\n",
    "        projected_embed = self.projection(embed)\n",
    "        replicated_embed = projected_embed.repeat(4, 4, 1, 1).permute(2,  3, 0, 1)\n",
    "        hidden_concat = torch.cat([inp, replicated_embed], 1)\n",
    "\n",
    "        return hidden_concat\n",
    "\n",
    "\n",
    "'''class minibatch_discriminator(nn.Module):\n",
    "    def __init__(self, num_channels, B_dim, C_dim):\n",
    "        super(minibatch_discriminator, self).__init__()\n",
    "        self.B_dim = B_dim\n",
    "        self.C_dim =C_dim\n",
    "        self.num_channels = num_channels\n",
    "        T_init = torch.randn(num_channels * 4 * 4, B_dim * C_dim) * 0.1\n",
    "        self.T_tensor = nn.Parameter(T_init, requires_grad=True)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        inp = inp.view(-1, self.num_channels * 4 * 4)\n",
    "        M = inp.mm(self.T_tensor)\n",
    "        M = M.view(-1, self.B_dim, self.C_dim)\n",
    "\n",
    "        op1 = M.unsqueeze(3)\n",
    "        op2 = M.permute(1, 2, 0).unsqueeze(0)\n",
    "\n",
    "        output = torch.sum(torch.abs(op1 - op2), 2)\n",
    "        output = torch.sum(torch.exp(-output), 2)\n",
    "        output = output.view(M.size(0), -1)\n",
    "\n",
    "        output = torch.cat((inp, output), 1)\n",
    "\n",
    "        return output\n",
    "'''\n",
    "\n",
    "class Utils(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def smooth_label(tensor, offset):\n",
    "        return tensor + offset\n",
    "\n",
    "    @staticmethod\n",
    "\n",
    "    # based on:  https://github.com/caogang/wgan-gp/blob/master/gan_cifar10.py\n",
    "#     def compute_GP(netD, real_data, real_embed, fake_data, LAMBDA):\n",
    "#         BATCH_SIZE = real_data.size(0)\n",
    "#         alpha = torch.rand(BATCH_SIZE, 1)\n",
    "#         alpha = alpha.expand(BATCH_SIZE, int(real_data.nelement() / BATCH_SIZE)).contiguous().view(BATCH_SIZE, 3, 64, 64)\n",
    "#         alpha = alpha.cuda()\n",
    "\n",
    "#         interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "\n",
    "#         interpolates = interpolates.cuda()\n",
    "\n",
    "#         interpolates = autograd.Variable(interpolates, requires_grad=True)\n",
    "\n",
    "#         disc_interpolates, _ = netD(interpolates, real_embed)\n",
    "\n",
    "#         gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n",
    "#                                   grad_outputs=torch.ones(disc_interpolates.size()).cuda(),\n",
    "#                                   create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "\n",
    "#         gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * LAMBDA\n",
    "\n",
    "#         return gradient_penalty\n",
    "        \n",
    "    @staticmethod\n",
    "    def save_checkpoint(netD, netG, dir_path, subdir_path, epoch):\n",
    "        path =  os.path.join(dir_path, subdir_path)\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "        torch.save(netD.state_dict(), '{0}/disc_{1}.pth'.format(path, epoch))\n",
    "        torch.save(netG.state_dict(), '{0}/gen_{1}.pth'.format(path, epoch))\n",
    "\n",
    "    @staticmethod\n",
    "    def weights_init(m):\n",
    "        classname = m.__class__.__name__\n",
    "        if classname.find('Conv') != -1:\n",
    "            m.weight.data.normal_(0.0, 0.02)\n",
    "        elif classname.find('BatchNorm') != -1:\n",
    "            m.weight.data.normal_(1.0, 0.02)\n",
    "            m.bias.data.fill_(0)\n",
    "\n",
    "\n",
    "class Logger(object):\n",
    "    def __init__(self, vis_screen):\n",
    "        self.viz = VisdomPlotter(env_name=vis_screen)\n",
    "        self.hist_D = []\n",
    "        self.hist_G = []\n",
    "        self.hist_Dx = []\n",
    "        self.hist_DGx = []\n",
    "\n",
    "    def log_iteration_wgan(self, epoch, gen_iteration, d_loss, g_loss, real_loss, fake_loss):\n",
    "        print(\"Epoch: %d, Gen_iteration: %d, d_loss= %f, g_loss= %f, real_loss= %f, fake_loss = %f\" %\n",
    "              (epoch, gen_iteration, d_loss.data.cpu().mean(), g_loss.data.cpu().mean(), real_loss, fake_loss))\n",
    "        self.hist_D.append(d_loss.data.cpu().mean())\n",
    "        self.hist_G.append(g_loss.data.cpu().mean())\n",
    "        \n",
    "    def log_iteration_gan(self, epoch, d_loss, g_loss, real_score, fake_score):\n",
    "        print(\"Epoch: %d, d_loss= %f, g_loss= %f, D(X)= %f, D(G(X))= %f\" % (\n",
    "            epoch, d_loss.data.cpu().mean(), g_loss.data.cpu().mean(), real_score.data.cpu().mean(),\n",
    "            fake_score.data.cpu().mean()))\n",
    "        self.hist_D.append(d_loss.data.cpu().mean())\n",
    "        self.hist_G.append(g_loss.data.cpu().mean())\n",
    "        self.hist_Dx.append(real_score.data.cpu().mean())\n",
    "        self.hist_DGx.append(fake_score.data.cpu().mean())\n",
    "\n",
    "    def plot_epoch(self, epoch):\n",
    "        self.viz.plot('Discriminator', 'train', epoch, np.array(self.hist_D).mean())\n",
    "        self.viz.plot('Generator', 'train', epoch, np.array(self.hist_G).mean())\n",
    "        self.hist_D = []\n",
    "        self.hist_G = []\n",
    "\n",
    "    def plot_epoch_w_scores(self, epoch):\n",
    "        self.viz.plot('Discriminator', 'train', epoch, np.array(self.hist_D).mean())\n",
    "        self.viz.plot('Generator', 'train', epoch, np.array(self.hist_G).mean())\n",
    "        self.viz.plot('D(X)', 'train', epoch, np.array(self.hist_Dx).mean())\n",
    "        self.viz.plot('D(G(X))', 'train', epoch, np.array(self.hist_DGx).mean())\n",
    "        self.hist_D = []\n",
    "        self.hist_G = []\n",
    "        self.hist_Dx = []\n",
    "        self.hist_DGx = []\n",
    "\n",
    "    def draw(self, right_images, fake_images):\n",
    "        self.viz.draw('generated images', fake_images.data.cpu().numpy()[:64] * 128 + 128)\n",
    "        self.viz.draw('real images', right_images.data.cpu().numpy()[:64] * 128 + 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
